{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing data/reut2-000.sgm begin~~~\n",
      "preprocessing data/reut2-001.sgm begin~~~\n",
      "preprocessing data/reut2-002.sgm begin~~~\n",
      "preprocessing data/reut2-003.sgm begin~~~\n",
      "preprocessing data/reut2-004.sgm begin~~~\n",
      "preprocessing data/reut2-005.sgm begin~~~\n",
      "preprocessing data/reut2-006.sgm begin~~~\n",
      "preprocessing data/reut2-007.sgm begin~~~\n",
      "preprocessing data/reut2-008.sgm begin~~~\n",
      "preprocessing data/reut2-009.sgm begin~~~\n",
      "preprocessing data/reut2-010.sgm begin~~~\n",
      "preprocessing data/reut2-011.sgm begin~~~\n",
      "preprocessing data/reut2-012.sgm begin~~~\n",
      "preprocessing data/reut2-013.sgm begin~~~\n",
      "preprocessing data/reut2-014.sgm begin~~~\n",
      "preprocessing data/reut2-015.sgm begin~~~\n",
      "preprocessing data/reut2-016.sgm begin~~~\n",
      "preprocessing data/reut2-017.sgm begin~~~\n",
      "preprocessing data/reut2-018.sgm begin~~~\n",
      "preprocessing data/reut2-019.sgm begin~~~\n",
      "preprocessing data/reut2-020.sgm begin~~~\n",
      "preprocessing data/reut2-021.sgm begin~~~\n",
      "\n",
      "read data:  2.396456241607666 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "data = {}\n",
    "new_id = -1   # current id\n",
    "topic_flag = False   # topic has value or not\n",
    "body_flag = False   # body has value or not\n",
    "body = ''\n",
    "id_list = []   # all id\n",
    "\n",
    "begin = time.time()\n",
    "filename = [\"data/reut2-%03d.sgm\" % r for r in range(0, 22)]\n",
    "for fn in filename:\n",
    "    print('preprocessing ' + fn + ' begin~~~')\n",
    "    file = open(fn, 'r', encoding = 'ISO-8859-1')\n",
    "    line = file.readlines()\n",
    "\n",
    "    for l in line:\n",
    "        if l.find('REUTERS TOPICS') != -1:\n",
    "            if topic_flag:   # 去除前一篇有topic沒有body的部分\n",
    "                del data[new_id]\n",
    "                new_id = -1\n",
    "                topic_flag = False\n",
    "                \n",
    "            dtype = re.split('[=]*[\"]+[ ]*', l)\n",
    "            if dtype[1] == 'YES':   # 判斷REUTERS TOPICS\n",
    "                new_id = dtype[9]   # new_id\n",
    "                data[new_id] = {}\n",
    "                data[new_id]['type'] = dtype[3]   # LEWISSPLIT => TRAIN or TEST\n",
    "                \n",
    "        elif l.find('<TOPICS><D>') != -1 and new_id != -1:\n",
    "            topic_flag = True\n",
    "            kind = re.split('<TOPICS><D>|</D><D>|</D></TOPICS>\\n', l)   # 切出每個topic\n",
    "            kind.pop()\n",
    "            kind.pop(0)\n",
    "            data[new_id]['category'] = kind   # topic category\n",
    "            \n",
    "        elif l.find('<BODY>') != -1 and new_id != -1:\n",
    "    #         去除沒有topic label的部分\n",
    "            if not topic_flag:\n",
    "                del data[new_id]\n",
    "                new_id = -1\n",
    "                continue\n",
    "                \n",
    "            body_flag = True\n",
    "            body = re.split('<BODY>', l)[1].strip()   # body的第一行\n",
    "            continue\n",
    "            \n",
    "        elif l.find('</BODY>') != -1 and new_id != -1 and topic_flag:  # body的最後一行\n",
    "            data[new_id]['body'] = body\n",
    "            id_list.append(new_id)    # store the new id\n",
    "            body_flag = False\n",
    "            topic_flag = False\n",
    "            new_id = -1\n",
    "            body = ''\n",
    "\n",
    "        if body_flag and topic_flag:\n",
    "            body += l.strip()   # 讀取body每一行並連接在一起\n",
    "end = time.time()\n",
    "print('\\nread data: ', end - begin, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tf(text):\n",
    "    lowers = text.lower()   # 轉小寫\n",
    "    \n",
    "#     remove punctuatuin\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    no_punctuation = lowers.translate(remove_punctuation_map)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(no_punctuation)   # string to token\n",
    "    \n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]   # stopword remove\n",
    "    \n",
    "    stemmer = PorterStemmer()   # stemming\n",
    "    stemmed = stem(filtered, stemmer)\n",
    "    \n",
    "    count = Counter(stemmed)   # count the token\n",
    "    \n",
    "    return dict(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = {}\n",
    "train_count = 0   # 訓練集數量\n",
    "\n",
    "begin = time.time()\n",
    "file = open('data/all-topics-strings.lc.txt', 'r')   # read the topic file\n",
    "line = file.readlines()\n",
    "\n",
    "# bag initial\n",
    "for l in line:\n",
    "    bag[l.strip()] = {}\n",
    "    bag[l.strip()]['body'] = ''   # 此topic的所有文章tf\n",
    "    bag[l.strip()]['count'] = 0   # 此topic包含多少篇文章\n",
    "\n",
    "for i in id_list:\n",
    "    if data[i]['type'] == 'TRAIN':\n",
    "        for t in data[i]['category']:\n",
    "            bag[t]['body'] += ' ' + data[i]['body']   # 合併所有屬於此topic的文章\n",
    "            bag[t]['count'] += 1\n",
    "            train_count += 1\n",
    "            \n",
    "for key in bag.keys():\n",
    "    count = 0\n",
    "    if bag[key]['body'] != '':\n",
    "        bag[key]['body'] = tf(bag[key]['body'])   # tf\n",
    "        for word in bag[key]['body'].keys():\n",
    "            count += bag[key]['body'][word]\n",
    "    bag[key]['allcount'] = count   # 此topic總字數\n",
    "\n",
    "# 若此topic沒有任何內容則將其去除，以免影響後續計算\n",
    "for key in list(bag):\n",
    "    if bag[key]['count'] == 0:\n",
    "        del bag[key]\n",
    "        \n",
    "end = time.time()\n",
    "print('bag construct: ', end - begin, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(test_data):\n",
    "    f_score = {}\n",
    "    \n",
    "    for topic in bag.keys():   # 對於每一個topic去算目前這個test的分數\n",
    "        occur = 0   # 計算目前這個test是否有詞不在此topic中，0表示沒有\n",
    "        score = 0\n",
    "        \n",
    "        for word in test_data.keys():\n",
    "            if word not in bag[topic]['body'].keys():\n",
    "                occur = 1\n",
    "                break\n",
    "        \n",
    "        for word in test_data.keys():\n",
    "            for tf in range(test_data[word]):\n",
    "#                 calculate the probability\n",
    "#                 P(B|A)P(A)\n",
    "#                 由於分母皆相同，所以將其去除並不影響結果\n",
    "                if word in bag[topic]['body'].keys():\n",
    "                    score += math.log((bag[topic]['body'][word] + occur) / (bag[topic]['allcount'] + occur))\n",
    "                else:\n",
    "                    score += math.log(occur/(bag[topic]['allcount'] + occur))\n",
    "                    \n",
    "        if bag[topic]['count'] != 0:\n",
    "            score += len(test_data) * math.log(bag[topic]['count'] / train_count)\n",
    "        else:\n",
    "            score = 0\n",
    "        \n",
    "        f_score[topic] = score   # 此test在所有topic的score\n",
    "        \n",
    "    return max(f_score, key=f_score.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count = 0   # 測試集數目\n",
    "correct = 0   # 預測成功數目\n",
    "test_actually = []   # 實際結果\n",
    "test_predict = []   # 預測結果\n",
    "\n",
    "begin = time.time()\n",
    "for i in id_list:\n",
    "    if data[i]['type'] == 'TEST':\n",
    "        data[i]['body'] = tf(data[i]['body'])   # tf\n",
    "        topic = naive_bayes(data[i]['body'])   # predict topic\n",
    "        test_count += 1\n",
    "        \n",
    "        if topic in data[i]['category']:   # predict correct\n",
    "            correct += 1\n",
    "            test_actually.append(topic)\n",
    "        else:   # predict incorrect\n",
    "            test_actually.append(data[i]['category'][0])\n",
    "        test_predict.append(topic)\n",
    "\n",
    "end = time.time()\n",
    "print('testing set preprocessing and predict: ', end - begin, 's\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = precision_score(test_actually, test_predict, average='macro')\n",
    "r = recall_score(test_actually, test_predict, average='macro')\n",
    "f1 = f1_score( test_actually, test_predict, average='macro' )\n",
    "\n",
    "print('precision: ', p)\n",
    "print('recall: ', r)\n",
    "print('F-measure: ', f1)\n",
    "print('accuracy: ', (correct / test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
