{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing data/reut2-000.sgm begin~~~\n",
      "preprocessing data/reut2-001.sgm begin~~~\n",
      "preprocessing data/reut2-002.sgm begin~~~\n",
      "preprocessing data/reut2-003.sgm begin~~~\n",
      "preprocessing data/reut2-004.sgm begin~~~\n",
      "preprocessing data/reut2-005.sgm begin~~~\n",
      "preprocessing data/reut2-006.sgm begin~~~\n",
      "preprocessing data/reut2-007.sgm begin~~~\n",
      "preprocessing data/reut2-008.sgm begin~~~\n",
      "preprocessing data/reut2-009.sgm begin~~~\n",
      "preprocessing data/reut2-010.sgm begin~~~\n",
      "preprocessing data/reut2-011.sgm begin~~~\n",
      "preprocessing data/reut2-012.sgm begin~~~\n",
      "preprocessing data/reut2-013.sgm begin~~~\n",
      "preprocessing data/reut2-014.sgm begin~~~\n",
      "preprocessing data/reut2-015.sgm begin~~~\n",
      "preprocessing data/reut2-016.sgm begin~~~\n",
      "preprocessing data/reut2-017.sgm begin~~~\n",
      "preprocessing data/reut2-018.sgm begin~~~\n",
      "preprocessing data/reut2-019.sgm begin~~~\n",
      "preprocessing data/reut2-020.sgm begin~~~\n",
      "preprocessing data/reut2-021.sgm begin~~~\n",
      "\n",
      "read data:  2.205906629562378 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "data = {}\n",
    "new_id = -1   # current id\n",
    "topic_flag = False   # topic has value or not\n",
    "body_flag = False   # body has value or not\n",
    "body = ''\n",
    "id_list = []   # all id\n",
    "\n",
    "begin = time.time()\n",
    "filename = [\"data/reut2-%03d.sgm\" % r for r in range(0, 22)]\n",
    "for fn in filename:\n",
    "    print('preprocessing ' + fn + ' begin~~~')\n",
    "    file = open(fn, 'r', encoding = 'ISO-8859-1')\n",
    "    line = file.readlines()\n",
    "\n",
    "    for l in line:\n",
    "        if l.find('REUTERS TOPICS') != -1:\n",
    "            if topic_flag:   # 去除前一篇有topic沒有body的部分\n",
    "                del data[new_id]\n",
    "                new_id = -1\n",
    "                topic_flag = False\n",
    "                \n",
    "            dtype = re.split('[=]*[\"]+[ ]*', l)\n",
    "            if dtype[1] == 'YES':   # 判斷REUTERS TOPICS\n",
    "                new_id = dtype[9]   # new_id\n",
    "                data[new_id] = {}\n",
    "                data[new_id]['type'] = dtype[3]   # LEWISSPLIT => TRAIN or TEST\n",
    "                \n",
    "        elif l.find('<TOPICS><D>') != -1 and new_id != -1:\n",
    "            topic_flag = True\n",
    "            kind = re.split('<TOPICS><D>|</D><D>|</D></TOPICS>\\n', l)   # 切出每個topic\n",
    "            kind.pop()\n",
    "            kind.pop(0)\n",
    "            data[new_id]['category'] = kind   # topic category\n",
    "            \n",
    "        elif l.find('<BODY>') != -1 and new_id != -1:\n",
    "    #         去除沒有topic label的部分\n",
    "            if not topic_flag:\n",
    "                del data[new_id]\n",
    "                new_id = -1\n",
    "                continue\n",
    "                \n",
    "            body_flag = True\n",
    "            body = re.split('<BODY>', l)[1].strip()   # body的第一行\n",
    "            continue\n",
    "            \n",
    "        elif l.find('</BODY>') != -1 and new_id != -1 and topic_flag:  # body的最後一行\n",
    "            data[new_id]['body'] = body\n",
    "            id_list.append(new_id)    # store the new id\n",
    "            body_flag = False\n",
    "            topic_flag = False\n",
    "            new_id = -1\n",
    "            body = ''\n",
    "\n",
    "        if body_flag and topic_flag:\n",
    "            body += l.strip()   # 讀取body每一行並連接在一起\n",
    "end = time.time()\n",
    "print('\\nread data: ', end - begin, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tf(text):\n",
    "    lowers = text.lower()   # 轉小寫\n",
    "    \n",
    "#     remove punctuatuin\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    no_punctuation = lowers.translate(remove_punctuation_map)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(no_punctuation)   # string to token\n",
    "    \n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]   # stopword remove\n",
    "    \n",
    "    stemmer = PorterStemmer()   # stemming\n",
    "    stemmed = stem(filtered, stemmer)\n",
    "    \n",
    "    count = Counter(stemmed)   # count the token\n",
    "    temp = dict(count)\n",
    "    \n",
    "#     1 + log(tf)\n",
    "    for key in list(temp):\n",
    "        temp[key] = 1 + math.log(temp[key])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test tf:  663.1301655769348 s\n",
      "idf:  0.382641077041626 s\n"
     ]
    }
   ],
   "source": [
    "idf = {}\n",
    "train_count = 0   # 訓練集數目\n",
    "\n",
    "start = time.time()\n",
    "# train and test set tf\n",
    "for i in id_list:\n",
    "    data[i]['body'] = tf(data[i]['body'])\n",
    "end = time.time()\n",
    "print('train and test tf: ', end - start, 's')\n",
    "\n",
    "# df\n",
    "start = time.time()    \n",
    "for i in id_list:\n",
    "    if data[i]['type'] == 'TRAIN':\n",
    "        train_count += 1\n",
    "        for key in data[i]['body'].keys():\n",
    "            if key in idf:\n",
    "                idf[key] += 1\n",
    "            else:\n",
    "                idf[key] = 1\n",
    "\n",
    "# idf = log(df)\n",
    "for key in list(idf):\n",
    "    idf[key] = math.log(train_count / idf[key])\n",
    "    \n",
    "end = time.time()\n",
    "print('idf: ', end - start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test tfidf:  0.6519205570220947 s\n"
     ]
    }
   ],
   "source": [
    "# tfidf\n",
    "start = time.time()\n",
    "for i in id_list:\n",
    "    for key in data[i]['body'].keys():\n",
    "        if key in idf:\n",
    "            data[i]['body'][key] *= idf[key]\n",
    "        else:   # if this word not in the train set, idf = 0\n",
    "            data[i]['body'][key] = 0\n",
    "end = time.time()\n",
    "print('train and test tfidf: ', end - start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the train set tfidf\n",
    "def combine(a, b):\n",
    "    x, y = Counter(a), Counter(b)\n",
    "    return dict(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train the centroid vector:  100.9134087562561 s\n"
     ]
    }
   ],
   "source": [
    "bag = {}\n",
    "\n",
    "file = open('data/all-topics-strings.lc.txt', 'r')   # read the topic file\n",
    "line = file.readlines()\n",
    "\n",
    "# bag initial\n",
    "for l in line:\n",
    "    bag[l.strip()] = {}\n",
    "    bag[l.strip()]['body'] = ''   # 此topic的所有文章tfidf\n",
    "    bag[l.strip()]['count'] = 0   # 此topic包含多少篇文章\n",
    "    \n",
    "start = time.time()\n",
    "for i in id_list:\n",
    "    if data[i]['type'] == 'TRAIN':\n",
    "        for t in data[i]['category']:\n",
    "            bag[t]['body'] = combine(bag[t]['body'], data[i]['body'])   # 合併所有屬於此topic的文章的tfidf\n",
    "            bag[t]['count'] += 1\n",
    "\n",
    "for key in bag.keys():\n",
    "    for word in list(bag[key]['body']):\n",
    "        bag[key]['body'][word] = bag[key]['body'][word] / bag[key]['count']   # 將tfidf和平均\n",
    "end = time.time()\n",
    "print('train the centroid vector: ', end - start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm(q):\n",
    "    ans = {}\n",
    "    for t in bag.keys():\n",
    "        dot_product = 0\n",
    "        doc_V = 0\n",
    "        query_V = 0\n",
    "        for key in q.keys():\n",
    "            if key in bag[t]['body']:\n",
    "                dot_product += (q[key] * bag[t]['body'][key])\n",
    "            query_V += (q[key] * q[key])\n",
    "        query_V = math.sqrt(query_V)\n",
    "        \n",
    "        for key in list(bag[t]['body']):\n",
    "            doc_V += (bag[t]['body'][key] * bag[t]['body'][key])\n",
    "        doc_V = math.sqrt(doc_V)\n",
    "        \n",
    "        if doc_V == 0 or query_V == 0:\n",
    "            ans[t] = 0\n",
    "        else:\n",
    "            ans[t] = (round(dot_product / (doc_V * query_V), 12))\n",
    "            \n",
    "    return max(ans, key=ans.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict:  398.45625257492065 s\n"
     ]
    }
   ],
   "source": [
    "test_count = 0   # 測試集數目\n",
    "correct = 0   # 預測成功數目\n",
    "test_actually = []   # 實際結果\n",
    "test_predict = []   # 預測結果\n",
    "\n",
    "start = time.time()\n",
    "for i in id_list:\n",
    "    if data[i]['type'] == 'TEST':\n",
    "        topic = vsm(data[i]['body'])\n",
    "        test_count += 1\n",
    "        \n",
    "        if topic in data[i]['category']:   # predict correct\n",
    "            correct += 1\n",
    "            test_actually.append(topic)\n",
    "        else:   # predict incorrect\n",
    "            test_actually.append(data[i]['category'][0])\n",
    "        test_predict.append(topic)\n",
    "end = time.time()\n",
    "print('predict: ', end - start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.6519652779805617\n",
      "recall:  0.6471236170237964\n",
      "F-measure:  0.6294657662924553\n",
      "accuracy:  0.8975200583515682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MI\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\MI\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "p = precision_score(test_actually, test_predict, average='macro')\n",
    "r = recall_score(test_actually, test_predict, average='macro')\n",
    "f1 = f1_score( test_actually, test_predict, average='macro' )\n",
    "\n",
    "print('precision: ', p)\n",
    "print('recall: ', r)\n",
    "print('F-measure: ', f1)\n",
    "print('accuracy: ', (correct / test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
